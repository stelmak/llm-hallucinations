# llm-hallucinations

**Research Directions:**

- Preventing Hallucinations in Low-Resource Conditions
- Using Synthetic Datasets to Prevent Hallucinations

**Methodology**
- investigate methods to prevent hallucinations in machine learning models under low-resource conditions
- create a synthetic dataset where models violate predefined guardrails that describe their role
- apply embedding modeling and topic modeling to explore the effectiveness of different techniques
- compare our approach with existing methods and provide a thorough analysis of our results

**Related Work**
 - [https://ar5iv.org/abs/2407.13193](Retrieval-Augmented Generation for Natural Language Processing: A Survey)
 - [https://arxiv.org/abs/2403.01548](In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation)
 - [https://arxiv.org/abs/2401.06102](Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models)
 - [https://arxiv.org/pdf/2402.09733](Do LLMs Know about Hallucination? An Empirical Investigation of LLMâ€™s Hidden States)
